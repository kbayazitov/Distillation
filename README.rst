.. class:: center

    :Название исследуемой задачи: Дистилляция моделей на многодоменных выборках
    :Тип научной работы: Дипломная работа
    :Автор: Баязитов Камил Маратович
    :Научный руководитель: д.ф.-м.н., Семенов Виталий Адольфович
    :Научный консультант(при наличии): к.ф.-м.н., Грабовой Андрей Валериевич

Abstract
========

Исследуется проблема понижения сложности аппроксимирующей модели при переносе на новые данные меньшей мощности. Вводятся понятия учителя, ученика для разных наборов данных. При этом мощность одного набора данных больше мощности другого. Рассматриваются методы, основанные на дистилляции моделей машинного обучения. Вводится предположение, что решение оптимизационной задачи от параметров обеих моделей и доменов повышает качество модели ученика. Проводится вычислительный эксперимент на реальных и синтетических данных.


Software modules developed as part of the study
======================================================
1. A python package *mylib* with all implementation `here <https://github.com/kbayazitov/distillation/tree/master/src>`_.
2. A code with all experiment visualisation `here <https://github.com/kbayazitov/distillation/blob/master/code/main.ipynb>`_. Can use `colab <http://colab.research.google.com/github/kbayazitov/distillation/blob/master/code/main.ipynb>`_.
