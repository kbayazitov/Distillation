\newpage


\begin{thebibliography}{99}
	\bibitem{Hinton2015}
	\textit{Hinton G., Vinyals O., Dean J} Distilling the Knowledge in a Neural Network // NIPS Deep Learning and Representation Learning Workshop. — 2015.
    
    \bibitem{Vapnik2016}
    \textit{D. Lopez-Paz, L. Bottou, B. Schölkopf, V. Vapnik} Unifying distillation and privileged information // ICLR. — 2016.
	
	\bibitem{KimRush2016}
	\textit{Yoon Kim, Alexander M. Rush} Sequence-Level Knowledge Distillation. — 2016.
	
	\bibitem{MDASR}
	\textit{H.Kim, M. Lee, H.Lee, T.Kang, J.Lee, E.Yang, S.Hwang} Multi-domain Knowledge Distillation via Uncertainty-Matching for End-to-End ASR Models. — 2021.
	
	\bibitem{DeepvisDA}
	\textit{Mei Wang, Weihong Deng} Deep Visual Domain Adaptation: A Survey. — 2018.
	
	\bibitem{FMNIST}
    \textit{Xiao H., Rasul K., Vollgraf R.} Fashion-MNIST: a Novel Image Dataset for
    Benchmarking Machine Learning Algorithms. — 2017. https://arxiv.org/abs/1708.07747.
	
\end{thebibliography}
