\newpage


\begin{thebibliography}{99}
	\bibitem{Hinton2015}
	\textit{Hinton G., Vinyals O., Dean J} Distilling the Knowledge in a Neural Network // NIPS Deep Learning and Representation Learning Workshop. — 2015.
    
    \bibitem{Vapnik2016}
    \textit{D.Lopez-Paz, L.Bottou, B.Schölkopf, V.Vapnik} Unifying distillation and privileged information // ICLR. — 2016.
	
	\bibitem{KimRush2016}
	\textit{Yoon Kim, Alexander M.Rush} Sequence-Level Knowledge Distillation. — 2016.
	
	\bibitem{MDASR}
	\textit{H.Kim, M. Lee, H.Lee, T.Kang, J.Lee, E.Yang, S.Hwang} Multi-domain Knowledge Distillation via Uncertainty-Matching for End-to-End ASR Models. — 2021.
	
	\bibitem{DeepvisDA}
	\textit{Mei Wang, Weihong Deng} Deep Visual Domain Adaptation: A Survey. — 2018.
	
	\bibitem{FMNIST}
    \textit{Xiao H., Rasul K., Vollgraf R.} Fashion-MNIST: a Novel Image Dataset for
    Benchmarking Machine Learning Algorithms. — 2017. https://arxiv.org/abs/1708.07747.
    
    \bibitem{MNIST}
    \textit{LeCun Y., Cortes C.} MNIST handwritten digit database. --- 2010. http://yann.lecun.com/exdb/mnist/
    
    \bibitem{VAE}
    \textit{Diederik P.Kingma, M. Welling} Auto-Encoding Variational Bayes. --- 2014. https://arxiv.org/pdf/1312.6114.pdf
    
    \bibitem{image_to_image}
    \textit{Y. Pang, J. Lin, T. Qin} Image-to-Image Translation: Methods and 
    Applications. --- 2021.
    
    \bibitem{DA}
    \textit{S. Sankaranarayanan, Y. Balaji, A. Jain} Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation. --- 2018.
    
    \bibitem{Adam}
    \textit{Kingma D., Ba J.} Adam: A Method for Stochastic Optimization // ICLR. — 2015.
    
    \bibitem{UDA}
    \textit{Hongruixuan Chen, Chen Wu, Yonghao Xu, Bo Du} Unsupervised Domain Adaptation for Semantic Segmentation via Low-level Edge Information Transfer. --- 2021.
    
    \bibitem{DA via prompt learning}
    \textit{Chunjiang Ge, Rui Huang, Mixue Xie, Zihang Lai} Domain Adaptation via Prompt Learning. --- 2022.
    
    \bibitem{Model compression}
    \textit{Zhiyuan Wu, Yu Jiang,  Minghao Zhao, Chupeng Cui} Spirit Distillation: A Model Compression Method with Multi-domain Knowledge Transfer
    
    \bibitem{UDAB}
    \textit{Y.Ganin, V.Lempitsky} Unsupervised Domain Adaptation by Backpropagation. --- 2015.
    
    \bibitem{Multi Sourcse Distilling}
    \textit{Sicheng Zhao, Guangzhi Wang, Shanghang Zhang, Yang Gu} Multi-source Distilling Domain Adaptation. --- 2020.
    
    \bibitem{DA through distillation}
    \textit{Brady Zhou, Nimit Kalra,  Philipp Krahenbuhl} Domain Adaptation Through Task Distillation. --- 2020.
    
    \bibitem{Object Detection}
    \textit{Guobin Chen, Wongun Choi, Xiang Yu} Learning Efficient Object Detection Models with Knowledge Distillation. --- 2017.
    
    \bibitem{Improved Distillation}
    \textit{Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li} Improved Knowledge Distillation via Teacher Assistant. --- 2020.
    
    \bibitem{Semantic Segmentation}
    \textit{Yifan Liu, Ke Chen, Chris Liu} Structured Knowledge Distillation for Semantic Segmentation. --- 2018.
    
    \bibitem{Acoustic Models}
    \textit{T. Asami, R. Masumura, Y.Yamaguchi} Domain adaptation of DNN acoustic models using knowledge distillation. --- 2017.
    
    \bibitem{Transfer Learning}
    \textit{Srikanth Tammina} Transfer learning using VGG-16 with Deep
    Convolutional Neural Network for Classifying Images. --- 2019.
    
    \bibitem{GAN}
    \textit{Antonia Creswell, Tom White, Vincent Dumoulin} Generative Adversarial Networks: An Overview. --- 2017.
    
    \bibitem{Grabovoy2022}
    \textit{Грабовой А.В., Стрижов В.В.} Вероятностная интерпретация задачи дистилляции // Автоматика и телемеханика, 2022.
    
    \bibitem{Grabovoy2021}
    \textit{Грабовой А.В., Стрижов В.В.} Байесовская дистилляция моделей глубокого обучения // Автоматика и телемеханика, 2021.
    
    \bibitem{Github}
    \textit{Код эксперимента}\\ https://github.com/kbayazitov/distillation/blob/main/code/main.ipynb
	
\end{thebibliography}
