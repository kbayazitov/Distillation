\newpage


\section{Введение}

\paragraph{Актуальность темы.} Традиционно алгоритмы машинного обучения разрабатываются для отдельной задачи, аппроксимирующей заданную выборку. Сбор и обработка наборов данных для каждой новой задачи и области являются чрезвычайно дорогими и трудоемкими процессами, и не всегда могут быть доступны достаточные данные для обучения. К тому же для каждой новой задачи сложную модель необходимо перестраивать с нуля. В этом случае можно использовать перенос информации с более сложной модели \textit{учителя} на модель \textit{ученика} с меньшим числом параметров. С другой стороны снижение сложности модели --- приоритетная задача, необходимая для повышения интерпретируемости моделей.

\paragraph{Цель работы.} Одним из способов повышения качества алгоритма машинного обучения является использование модели с большим числом параметров, ответы которой можно использовать при обучении модели с меньшим числом параметров. Модели с меньшим числом параметров являются более интерпретируемыми. Цель данной работы заключается в снижении сложности модели машинного обучения, а также переходе к данным меньшей мощности. Для этого предлагается использовать два основных метода --- дистилляция моделей и доменная адаптация.

\paragraph{Новизна.} Предложен метод для случая, когда модели учителя и ученика заданы на выборках разной мощности из разных, но схожих генеральных совокупностей. При чем задано отображение с выборки меньшей мощности в выборку большей мощности.

\subsection{Обзор предметной области}

\begin{definition}
Учитель --- модель с большим числом параметров, ответы которой используются при обучении модели ученика.
\end{definition}

\begin{definition}
Ученик --- модель с меньшим числом параметров, при обучении которой используются ответы модели учителя.
\end{definition}

\begin{definition}
Дистилляция модели --- выбор модели с меньшим числом параметров из параметрического семейства функций согласно решению оптимизационной задачи с учетом ответов модели с большим числом параметров.
\end{definition}

\begin{definition}
Привилегированные признаки --- набор признаков, доступных только на этапе обучения модели.
\end{definition}

Дистилляция моделей машинного обучения использует метки модели с большим числом параметров для обучения модели с меньшим числом параметров. В~\cite{Hinton2015} рассматривается метод дистилляции, предложенной Джеффри Хинтоном, с учетом меток учителя при помощи функции \text{softmax} с параметром температуры, а в~\cite{Vapnik2016} рассматривается объединение методов дистилляци, предложенной Джеффри Хинтоном, и привилегированной информации~\cite{Vapnik2016}, предложенной Владимиром Наумовичем Вапником, в обобщенную дистилляцию. В вероятностной дистилляции вводится гипотеза порождения выборки совместно с ответами учителя. В работе~\cite{Grabovoy2021} рассмотрена байесовская дистилляция моделей глубокого обучения, в рамках которой вместе с ответами учителя используется апостериорное распределение параметров модели учителя. На основе этого апостериорного распределения задается априорное распределение модели ученика. Дистиляцция моделей используется в широком классе задач. В~\cite{MDASR} рассматривается метод дистилляции моделей для задачи распознавания речи. В~\cite{Object Detection} рассматривается метод дистилляции моделей для задачи распознавания объектов с использованием взвешенной кросс-энтропии для улучшения качества на выборках с неслабансированными классами. В~\cite{Semantic Segmentation} рассматривается метод дистилляции моделей для задачи семантической сегментации с использованием GAN. В~\cite{Improved Distillation} предлагается усовершенствованный метод дистилляции с использованием помимо модели учителя также и модели помощника --- сети среднего размера между размерами учителя и ученика.

В задаче доменнной адаптации используются наборы данных, схожих между собой. В общем случае выборки состоят из объектов, которые можно разделить на домены из близких генеральных совокупностей~\cite{image_to_image, DA via prompt learning}. К примеру, можно составить отображение из множества реальных фотографий малой мощности во множество сгенерированных движком изображений, мощность которого естественно больше~\cite{DA,UDA}. Так, в~\cite{image_to_image, Transfer Learning} рассматриваются отображения, изменяющие стиль изображений. Одним из примеров генерации новых изображений является работа модели вариационного автокодировщика~\cite{VAE}, способного для одного и того же объекта строить вероятностное распределение, на основе которого можно получить целое семейство новых объектов. Другим примером может служить работа генеративной состязательной сети GAN~\cite{GAN}, в которой одновременно обучаются две модели. Модель генератора учится порождать новые объекты из шума, а модель дискриминатора учится отличать их от реальных объектов. Данные методы позволяют получить синтетически сгенерированную выборку из близкой генеральной совокупности к исходной выборке.

Различные постановки задач доменной адаптации описываются в~\cite{DeepvisDA}, встречаются постановки с частично размеченным целевым доменом и неразмеченным вовсе. Задачи с неразмеченным целевым доменом направлены на то, чтобы модели, обученные на синтетических данных, адаптировались к реальным данным~\cite{UDA}. Таким образом, доменная адаптация использует размеченные данные нескольких исходных доменов для выполнения новых задач в целевом домене.
В~\cite{MDASR} рассматривается метод доменной адаптации для задачи распознавания речи. В~\cite{Acoustic Models} рассматривается метод доменной адаптации для обучения акустических моделей на основе дистилляции моделей.

Одной из постановок задач доменной адаптации является перенос стиля изображений~\cite{image_to_image, Transfer Learning}.
Так, в~\cite{image_to_image} предлагается использовать селфи-изображения в качестве исходного домена и перевести их в изображения желаемого художественного стиля на основе выборки из требуемого стиля. Таким образом можно использовать синтетически сгенерированные данные для обучения.

В рассмотренных выше методах дистилляции~\cite{Hinton2015, Vapnik2016, Grabovoy2021} рассматривается случай, когда модели учителя и ученика аппроксимируют выборки из разных генеральных совокупностей. Для задачи дистилляции, предложенной Джеффри Хинтоном~\cite{Hinton2015}, исходный и целевой наборы данных совпадают.
Типичной задачей дистилляции моделей на многодоменных выборках является задача машинного перевода текстов, описанная в~\cite{KimRush2016}.


\subsection{Предложенный метод}

Предлагается при обучении модели ученика использовать помимо меток учителя на одном из доменов также и связь между доменами. При этом в качестве доменов должны служить близкие генеральные совокупности.

\begin{definition}
Генеральная совокупность объектов $B$ называется близкой к совокупности $A$, если существует инъективное отображение $\varphi: A \rightarrow B$
\end{definition}

Таким образом в качестве доменов разной мощности могут служить настоящие и сгенерированные изображения. В качестве отображений между изображениями рассматриваются нормальный шум, сверточные преобразования и генерация изображений с помощью модели вариацинного автокодировщика~\cite{VAE}. При этом исследуются отображения, для которых существования обратных не рассматриваются. Ожидается, что качество полученных моделей на одном домене будет превышать качество моделей, в обучении которых не использовались метки учителя на другом домене.

В качестве экспериментальных данных используются реальные данные и синтетическая выборка. В качестве реальных данных рассматривается выборка \text{Fashion-MNIST}~\cite{FMNIST}, состоящая из изображений одежды, и выборка \text{MNIST}~\cite{MNIST}, состоящая из изображений рукописных цифр.

