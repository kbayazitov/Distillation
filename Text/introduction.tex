\newpage


\section{Введение}

\paragraph{Актуальность темы.} Сбор и обработка наборов данных для каждой новой задачи и области являются чрезвычайно дорогими и трудоемкими процессами, и не всегда могут быть доступны достаточные данные для обучения. Поэтому снижение сложности модели --- приоритетная задача, необходимая для повышения интерпретируемости моделей.

\paragraph{Цель работы.} Одним из способов повышения качества алгоритма машинного обучения является использование модели с большим числом параметров, ответы которой можно использовать при обучении модели с меньшим числом параметров, более подходящей для развертывания. Цель данной работы заключается в понижении сложности модели машинного обучения при переходе к данным меньшей мощности. Для этого предлагается использовать два основных метода - дистилляция моделей и доменная адаптация.

\paragraph{Новизна.} Предложен подход для случая, когда модели учителя и ученика заданы на выборках разной мощности. При этом задана связь между выборками.

\subsection{Обзор предметной области}

Дистилляция моделей машинного обучения использует метки модели с большим числом параметров для обучения модели с меньшим числом параметров. В~\cite{Hinton2015} рассматривается метод дистилляции, предложенной Джеффри Хинтоном, с учетом меток учителя при помощи функции \text{softmax} с параметром температуры, а в~\cite{Vapnik2016} рассматривается объединение методов дистилляци, предложенной Джеффри Хинтоном, и привилегированной информации~\cite{Vapnik2016}, предложенной Владимиром Вапником, в обобщенную дистилляцию. Дистиляцция моделей используется в широком классе задач. В~\cite{MDASR} рассматривается метод дистилляции моделей для задачи распознавания речи.

 Выборки могут состоять из объектов, которые можно разделить на домены~\cite{image_to_image, DA via prompt learning}. К примеру, можно составить отображение из множества реальных фотографий малой мощности во множество сгенерированных движком изображений, мощность которого естественно больше~\cite{DA,UDA}. Так, в\cite{image_to_image} рассматриваются отображения, изменяющие стиль изображений, а в\cite{DA} предлагается использовать синтетически сгенерированные данные для обучения взамен ре. Одним из примеров генерации новых изображений является работа модели вариационного автокодировщика~\cite{VAE}, способного для одного и того же объекта строить вероятностное распределение, на основе которого можно получить целое семейство новых объектов. Для задачи дистилляции, предложенной Джеффри Хинтоном~\cite{Hinton2015}, исходный и целевой домены равны. Различные постановки задач доменной адаптации описываются в~\cite{DeepvisDA}, встречаются постановки с частично размеченным целевым доменом и неразмеченным вовсе. Таким образом, доменная адаптация использует размеченные данные нескольких исходных доменов для выполнения новых задач в целевом домене.
 
Типичной задачей дистилляции моделей на многодоменных выборках является задача машинного перевода текстов, описанная в~\cite{KimRush2016}.

\subsection{Предложенный метод}

Предлагается при обучении модели ученика использовать помимо меток учителя также и связь между доменами. Таким образом в качестве доменов разной мощности могут служить настоящие и сгенерированные изображения. В качестве отображений между изображениями рассматриваются нормальный шум, сверточные преобразования и генерация с помощью вариацинного автокодировщика~\cite{VAE}. При этом исследуются отображения, для которых существования обратных не рассматриваются. Ожидается, что качество полученных моделей будет превышать качество моделей, в обучении которых не использовались метки учителя.

В качестве экспериментальных данных используются реальные данные и синтетическая выборка. В качестве реальных данных рассматривается выборка \text{FashionMnist}~\cite{FMNIST}, состоящая из изображений одежды, для которой требуется решить задачу классификации на 10 типов одежды.\\

