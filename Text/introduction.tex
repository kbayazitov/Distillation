\newpage


\section{Введение}
\paragraph{Актуальность темы.} Одним из самых простых способов повышения качества алгоритма машинного обучения является использование ансамбля моделей. Однако сбор и обработка наборов данных для каждой новой задачи и области являются чрезвычайно дорогими и трудоемкими процессами, и не всегда могут быть доступны достаточные данные для обучения.
\paragraph{Цель работы.} Вместо ансамбля моделей можно использователь модель с большим числом параметров, ответы которой можно использовать при обучении модели с меньшим числом параметров, более подходящей для развертывания. Цель данной работы заключается в понижении сложности модели машинного обучения при переходе к домену меньшей мощности. Для этого предлагается использовать два основных метода - дистилляция моделей и доменная адаптация.
\paragraph{Новизна.} Предложен подход для случая, когда модели учителя и ученика заданы на выборках разной мощности. При этом задана связь между выборками.
\subsection{Обзор предметной области}
Дистилляция моделей машинного обучения использует метки модели с большим числом параметров для обучения модели с меньшим числом параметров. В~\cite{Hinton2015} рассматривается метод дистилляции, предложенной Дж.Хинтоном, с учетом меток учителя при помощи функции \text{softmax} с параметром температуры, а в~\cite{Vapnik2016} рассматривается объединение методов дистилляци, предложенной Дж.Хинтоном, и привилегированной информации~\cite{Vapnik2016}, предложенной В.Вапником, в обобщенную дистилляцию. Дистиляцция моделей используется в широком классе задач. В~\cite{MDASR} рассматривается метод дистилляции моделей для задачи распознавания речи.\\
Часто выборки могут состоять из объектов, которые можно разделить на домены~\cite{image_to_image}. К примеру, можно составить отображение из множества реальных фотографий малой мощности во множество сгенерированных движком изображений, мощность которого естественно больше~\cite{DA}. Одним из самых простых примеров генерации новых изображений является работа модели вариационного автокодировщика~\cite{VAE}, способного для одного и того же объекта строить вероятностное распределение, на основе которого можно получить целое семейство новых объектов. Для задачи дистилляции, предложенной Джеффри Хинтоном, исходный и целевой домены равны. Различные постановки задач доменной адаптации описываются в~\cite{DeepvisDA}, встречаются постановки с частично размеченным целевым доменом и неразмеченным вовсе. Таким образом, доменная адаптация использует размеченные данные нескольких исходных доменов для выполнения новых задач в целевом домене.\\
Типичной задачей дистилляции моделей на многодоменных выборках является задача машинного перевода текстов, описанная в~\cite{KimRush2016}.\\
\subsection{Предложенный метод}
Предлагается при обучении модели ученика использовать помимо меток учителя также и связь между доменами с возможностью выбора веса дистилляции. Таким образом в качестве доменов разной мощности могут служить настоящие и сгенерированные изображения.\\
В качестве экспериментальных данных используются реальные данные и синтетическая выборка. В качестве реальных данных рассматривается выборка \text{FashionMnist}~\cite{FMNIST}, состоящая из изображений одежды, для которой требуется решить задачу классификации на 10 типов одежды.\\

