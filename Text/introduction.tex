\newpage


\section{Введение}

\paragraph{Актуальность темы.} Традиционно алгоритмы машинного обучения разрабатываются для решения конкретной задачи. Поэтому, в зависимости от новой постановки задачи, модель необходимо перестраивать с нуля. Сбор и обработка наборов данных для каждой новой задачи и области являются чрезвычайно дорогими и трудоемкими процессами, и не всегда могут быть доступны достаточные данные для обучения. В этом случае можно использовать перенос знаний с большой модели на модель с меньшим числом параметров. С другой стороны снижение сложности модели --- приоритетная задача, необходимая для повышения интерпретируемости моделей.

\paragraph{Цель работы.} Одним из способов повышения качества алгоритма машинного обучения является использование модели с большим числом параметров, ответы которой можно использовать при обучении модели с меньшим числом параметров, более интерпретируемой. Цель данной работы заключается в понижении сложности модели машинного обучения при переходе к данным меньшей мощности. Для этого предлагается использовать два основных метода --- дистилляция моделей и доменная адаптация.

\paragraph{Новизна.} Предложен подход для случая, когда модели учителя и ученика заданы на выборках разной мощности из разных, но схожих генеральных совокупностей. При чем задано отображение с выборки меньшей мощности в выборку большей мощности.

\subsection{Обзор предметной области}

Дистилляция моделей машинного обучения использует метки модели с большим числом параметров для обучения модели с меньшим числом параметров. В~\cite{Hinton2015} рассматривается метод дистилляции, предложенной Джеффри Хинтоном, с учетом меток учителя при помощи функции \text{softmax} с параметром температуры, а в~\cite{Vapnik2016} рассматривается объединение методов дистилляци, предложенной Джеффри Хинтоном, и привилегированной информации~\cite{Vapnik2016}, предложенной Владимиром Наумовичем Вапником, в обобщенную дистилляцию. Дистиляцция моделей используется в широком классе задач. В~\cite{MDASR} рассматривается метод дистилляции моделей для задачи распознавания речи. В~\cite{Object Detection} рассматривается метод дистилляции моделей для задачи распознавания объектов с использованием взвешенной кросс-энтропии для устранения дисбаланса классов. В~\cite{Semantic Segmentation} рассматривается метод дистилляции моделей для задачи семантической сегментации с использованием GAN. В~\cite{Improved Distillation} предлагается усовершенствованный метод дистилляции с использованием помимо модели учителя также и модели помощника - сети среднего размера между размерами учителя и ученика.

Выборки могут состоять из объектов, которые можно разделить на домены~\cite{image_to_image, DA via prompt learning}. К примеру, можно составить отображение из множества реальных фотографий малой мощности во множество сгенерированных движком изображений, мощность которого естественно больше~\cite{DA,UDA}. Так, в~\cite{image_to_image, Transfer Learning} рассматриваются отображения, изменяющие стиль изображений. Одним из примеров генерации новых изображений является работа модели вариационного автокодировщика~\cite{VAE}, способного для одного и того же объекта строить вероятностное распределение, на основе которого можно получить целое семейство новых объектов. Для задачи дистилляции, предложенной Джеффри Хинтоном~\cite{Hinton2015}, исходный и целевой наборы данных совпадают.
 
Различные постановки задач доменной адаптации описываются в~\cite{DeepvisDA}, встречаются постановки с частично размеченным целевым доменом и неразмеченным вовсе. Таким образом, доменная адаптация использует размеченные данные нескольких исходных доменов для выполнения новых задач в целевом домене.
В~\cite{MDASR} рассматривается метод доменной адаптации для задачи распознавания речи. В~\cite{Acoustic Models} рассматривается метод доменной адаптации для обучения акустических моделей на основе дистилляции моделей.
 
Типичной задачей дистилляции моделей на многодоменных выборках является задача машинного перевода текстов, описанная в~\cite{KimRush2016}.

\subsection{Предложенный метод}

Предлагается при обучении модели ученика использовать помимо меток учителя также и связь между доменами. Таким образом в качестве доменов разной мощности могут служить настоящие и сгенерированные изображения. В качестве отображений между изображениями рассматриваются нормальный шум, сверточные преобразования и генерация с помощью вариацинного автокодировщика~\cite{VAE}. При этом исследуются отображения, для которых существования обратных не рассматриваются. Ожидается, что качество полученных моделей будет превышать качество моделей, в обучении которых не использовались метки учителя.

В качестве экспериментальных данных используются реальные данные и синтетическая выборка. В качестве реальных данных рассматривается выборка \text{FashionMnist}~\cite{FMNIST}, состоящая из изображений одежды, для которой требуется решить задачу классификации на 10 типов одежды.\\

