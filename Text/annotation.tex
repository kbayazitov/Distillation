\newpage


\begin{abstract}
Исследуется проблема понижения сложности аппроксимирующей модели при переносе модели к новым данным меньшей мощности. Вводятся понятия учителя, ученика для сильного и слабого доменов соответственно. Предполагается, что признаковые описания моделей ученика и учителя принадлежат разным доменам. При этом мощность одного домена больше мощности другого. Рассматриваются методы, основанные на дистилляции моделей машинного обучения. Вводится предположение, что решение оптимизационной задачи от параметров обеих моделей и доменов повышает качество модели ученика. Проводится вычислительный эксперимент на реальных и синтетических данных.


\smallskip
\textbf{Ключевые слова}: адаптация доменов, дистилляция, нейронные сети, обучение с учителем
\end{abstract}
